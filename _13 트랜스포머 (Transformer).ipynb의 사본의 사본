{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qBC_BPdmQgTWSd6n1B4d-pNAaATy_xVC","timestamp":1670229227025}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6PQSnsGeA2OH"},"source":["# 트랜스포머 (Transformer)\n","\n","* 참고: https://wikidocs.net/31379"]},{"cell_type":"markdown","metadata":{"id":"nbQ-h_XxBAiq"},"source":["* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n","* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n","* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RDiFPIdUBBS2"},"source":["## 포지셔널 인코딩"]},{"cell_type":"markdown","metadata":{"id":"rLqHf_4SEWoa"},"source":["* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n","* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n","* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n","* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"]},{"cell_type":"code","metadata":{"id":"SiO5c_HIFBAk","executionInfo":{"status":"ok","timestamp":1670257597501,"user_tz":-540,"elapsed":564,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def positional_encoding(dim, sentence_length):\n","  encoded_vec = np.array([pos / np.power(10000, 2 * i /dim) for pos in range(sentence_length) for i in range(dim)])\n","  encoded_vec[::2] = np.sin(encoded_vec[::2])\n","  encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n","  return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)"],"execution_count":480,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099gUUxhAgy3"},"source":["## 레이어 정규화"]},{"cell_type":"markdown","metadata":{"id":"XCdips98yPuH"},"source":["*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n","*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"]},{"cell_type":"code","metadata":{"id":"TSJjxF86Aeg3","executionInfo":{"status":"ok","timestamp":1670257597811,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def layer_norm(inputs, eps = 1e-6):\n","  feature_shape = inputs.get_shape()[-1:]\n","  mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n","  std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n","  beta = tf.Variable(tf.zeros(feature_shape), trainable=False)\n","  gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n","  return gamma * (inputs - mean) / (std + eps) + beta"],"execution_count":481,"outputs":[]},{"cell_type":"code","metadata":{"id":"km9ORxIun-MU","executionInfo":{"status":"ok","timestamp":1670257597811,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def sublayer_connection(inputs, sublayer, dropout=0.2):\n","  outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n","  return outputs"],"execution_count":482,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ppb7IxJ3diMC"},"source":["## 어텐션"]},{"cell_type":"markdown","metadata":{"id":"1JaU6MHgy9V2"},"source":["\n","\n","*   트랜스포머 모델의 핵심이 되는 부분\n","*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n","  1.   multi-head attention\n","      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n","      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n","      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n","  2.   self attention\n","      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n","      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n","      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n","      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n","\n","\n","\n","\n","*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRyL0KDXi6ej"},"source":["### scaled-dot product attention 구현"]},{"cell_type":"markdown","metadata":{"id":"6HtmcgRR3Cr-"},"source":["* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n","* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n","* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"]},{"cell_type":"code","metadata":{"id":"ALEMzi4fdiSQ","executionInfo":{"status":"ok","timestamp":1670257597811,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def scaled_dot_product_attention(query, key, value, masked=False):\n","  key_dim_size = float(key.get_shape().as_list()[-1])\n","  key = tf.transpose(key, perm=[0,2,1])\n","\n","  outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n","\n","  if masked:\n","    diag_vals = tf.ones_like(outputs[0,:,:])\n","    tril = tf.linalg.LinearOperatorBlockLowerTriangular(diag_vals).to_dense()\n","    masks = tf.tile(expand_dims(tril, 0),[tf.shape(outputs)[0], 1, 1])\n","    paddings = tf.ones_like(mask)*(-2**30)\n","    outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n","\n","  attention_map = tf.nn.softmax(outputs)\n","  return tf.matmul(attention_map, value)"],"execution_count":483,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yr20BxvVi-8b"},"source":["### multi-head attention 구현"]},{"cell_type":"markdown","metadata":{"id":"Gb5qflUH14-H"},"source":["* multi-head attention의 구현 과정\n","  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n","  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n","  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n","  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ooc3FAdQi_Gz","executionInfo":{"status":"ok","timestamp":1670257597812,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def multi_head_attention(query, key, value, num_units, heads, masked=False):\n","  query = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(query)\n","  key = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(key)\n","  value = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(value)\n","\n","  query = tf.concat(tf.split(query, heads, axis=-1), axis = 0)\n","  key = tf.concat(tf.split(key, heads, axis=-1), axis = 0)\n","  value = tf.concat(tf.split(value, heads, axis=-1), axis = 0)\n","\n","  attention_map = scaled_dot_product_attention(query, key, value, masked)\n","  attn_outputs = tf.concat(tf.split(attention_map, heads, axis=0), axis=-1)\n","  attn_outputs = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n","\n","  return attn_outputs"],"execution_count":484,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78Zn5-fYITD4"},"source":["## 포지션-와이즈 피드 포워드 신경망"]},{"cell_type":"markdown","metadata":{"id":"-xxeG2xvo3ZN"},"source":["\n","\n","*   multi-head attention의 결과인 행렬을 입력받아 연산\n","*   일반적인 완전 연결 신경망(Dense layer)를 사용\n","*   position-wise FFNN은 인코더와 디코더에 모두 존재\n","\n"]},{"cell_type":"code","metadata":{"id":"0tSFd5OaITJ0","executionInfo":{"status":"ok","timestamp":1670257597812,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def feed_forward(inputs, num_units):\n","  feature_shape = inputs.get_shape()[-1]\n","  inner_layer = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(inputs)\n","  outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n","  return outputs"],"execution_count":485,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuccViYgBK6v"},"source":["## 인코더\n"]},{"cell_type":"markdown","metadata":{"id":"tG3MH0n1JVLz"},"source":["* 인코더는 하나의 어텐션을 사용\n","  + encoder self-attention (multi-head self-attention과 동일)"]},{"cell_type":"code","metadata":{"id":"m5T0pzBoAnn3","executionInfo":{"status":"ok","timestamp":1670257597812,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def encoder_module(inputs, model_dim, ffn_dim, heads):\n","  self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n","  outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","  return outputs\n","\n","def encoder(inputs, model_dim, ffn_dim, heads, num_layer):\n","  outputs = inputs\n","  for i in range(num_layer):\n","    outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n","\n","  return outputs"],"execution_count":486,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcgHRcTEBQqg"},"source":["## 디코더"]},{"cell_type":"markdown","metadata":{"id":"cNj-6FLQwT4-"},"source":["* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n","  1. masked decoder self-attention\n","  2. encoder-decoder attention\n","  3. position-wise FFNN\n","\n","* 디코더에서는 2종류의 어텐션을 사용\n","  1.   masked decoder self-attention\n","    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n","    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n","    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n","  2.   encoder-decoder attention\n","    *   앞서 설명한 multi-head attention과 동일\n","\n"]},{"cell_type":"code","metadata":{"id":"2B05wr7aARcT","executionInfo":{"status":"ok","timestamp":1670257597812,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n","  masked_self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads, masked=True))\n","  self_attn = sublayer_connection(masked_self_attn, \n","                                  multi_head_attention(masked_self_attn,\n","                                                       encoder_outputs,\n","                                                       encoder_outputs, \n","                                                       model_dim, heads))\n","  outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","  return outputs\n","\n","def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layer):\n","  outputs = inputs\n","  for i in range(num_layer):\n","    outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n","\n","  return outputs\n","\n"],"execution_count":487,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtztlyUB1ERS"},"source":["## 트랜스포머를 활용한 챗봇"]},{"cell_type":"markdown","metadata":{"id":"6CGUIAzv6eWs"},"source":["### konlpy 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"Ae0mHT49v5gy"},"source":["*    한글을 처리하기 위해 konlpy 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"U8yf75uG6hBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670257601501,"user_tz":-540,"elapsed":3695,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}},"outputId":"6ce4e17f-35d0-417b-a791-aea2b067dc2a"},"source":["!pip install konlpy"],"execution_count":488,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rUMXvK5H1G9H"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"miXrjR316mNb"},"source":["* 처리에 필요한 각종 변수 선언\n","* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n","\n"]},{"cell_type":"code","metadata":{"id":"SMjn5PfE1GZR","executionInfo":{"status":"ok","timestamp":1670257601502,"user_tz":-540,"elapsed":13,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["import re\n","import tensorflow as tf\n","\n","filters = \"([~.,!?\\\"':;)(])\"\n","PAD = '<PADDING>'\n","STD = '<START>'\n","END = '<END>'\n","UNK = '<UNKNOWN>'\n","\n","PAD_INDEX = 0\n","STD_INDEX = 0\n","END_INDEX = 0\n","UNK_INDEX = 0\n","\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(filters)\n"],"execution_count":489,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmRFuH2r6oNJ"},"source":["* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"CmrmdXkePWYb","executionInfo":{"status":"ok","timestamp":1670257601502,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","def load_data(data_path):\n","  data_df = pd.read_csv(data_path, header=0)\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=111)\n","\n","  return train_input, train_label, eval_input,  eval_label"],"execution_count":490,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHuOJHPtPXqq"},"source":["* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"QtQL-AP06oSa","executionInfo":{"status":"ok","timestamp":1670257601502,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def load_vocabulary(data_path):\n","  data_df = pd.read_csv(data_path, encoding='utf-8')\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  if tokenize_as_morph:\n","    question = prepro_like_morphlized(question)\n","    answer = prepro_like_morphlized(answer)\n","\n","  data = []\n","  data.extend(question)\n","  data.extend(answer)\n","  words = data_tokenizer(data)\n","  words = list(set(words))\n","  words[:0] = MARKER\n","\n","\n","  char2idx = {char:idx for idx, char in enumerate(words)}\n","  idx2dix = {idx:char for idx, char in enumerate(words)}\n","  return char2idx,idx2dix, len(char2idx)"],"execution_count":491,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wYtpjv76r5q"},"source":["* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"-bQ3FOva6tg6","executionInfo":{"status":"ok","timestamp":1670257601502,"user_tz":-540,"elapsed":11,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["from konlpy.tag import Okt\n","\n","def prepro_like_morphlized(data):\n","  morph_analyzer = Okt()\n","  result_data = list()\n","  for seq in data:\n","    morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n","    result_data.append(morphlized_seq)\n","  return result_data"],"execution_count":492,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhsVp4pWPTR3"},"source":["* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"otLI_RUfPR_g","executionInfo":{"status":"ok","timestamp":1670257601502,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def data_tokenizer(data):\n","  words = []\n","  for sentence in data:\n","    sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","    for word in sentence.split():\n","      words.append(word)\n","  return [word for word in words if word]"],"execution_count":493,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OkKPA-Mx6uaC"},"source":["* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n","\n"]},{"cell_type":"code","source":["import numpy as np\n"],"metadata":{"id":"9xxG75qnZ4il","executionInfo":{"status":"ok","timestamp":1670257601503,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"execution_count":494,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK-yeSThPGsa","executionInfo":{"status":"ok","timestamp":1670257601503,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def enc_processing(value, dictionary):\n","  sequences_input_index = []\n","  sequences_length = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","    sequence_index = []\n","    for word in sequence.split():\n","      if dictionary.get(word) is not None:\n","        sequence_index.extend([dictionary[word]])\n","      else:\n","        sequence_index.extend([dictionary[UNK]])\n","    if len(sequence_index) > max_len:\n","      sequence_index = sequence_index[:max_len]\n","    sequences_length.append(len(sequence_index))\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_input_index.append(sequence_index)\n","  return np.asarray(sequences_input_index), sequences_length"],"execution_count":495,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4mM57_FPIg7"},"source":["* decoder의 입력을 구성하기 위한 함수 `dec_input_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"cX_NpcTq6vw6","executionInfo":{"status":"ok","timestamp":1670257601503,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def dec_output_processing(value, dictionary):\n","  sequences_output_index = []\n","  sequences_length = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","    sequence_index = []\n","    sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n","    if len(sequence_index) > max_len:\n","      sequence_index = sequence_index[:max_len]\n","    sequences_length.append(len(sequence_index))\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_output_index.append(sequence_index)\n","  return np.asarray(sequences_output_index), sequences_length"],"execution_count":496,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otsTEt4FPLJX"},"source":["* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"eeP0PWHEPMma","executionInfo":{"status":"ok","timestamp":1670257601503,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def dec_target_processing(value, dictionary):\n","  sequences_target_index = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","    sequence_index = [dictionary[word] for word in sequence.split()]\n","    if len(sequence_index) >= max_len:\n","      sequence_index = sequence_index[:max_len - 1] + [dictionary[END]]\n","    else:\n","      sequence_index += [dictionary[END]]\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_target_index.append(sequence_index)\n","  return np.asarray(sequences_target_index)"],"execution_count":497,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tb9vVUng6xDq"},"source":["* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n","* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n","* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n","\n"]},{"cell_type":"code","metadata":{"id":"uAlKV4xF62Uf","executionInfo":{"status":"ok","timestamp":1670257601503,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.from_tensor_slices((train_input_enc, train_output_enc, train_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearrange)\n","  dataset = dataset.repeat()\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","\n","def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_enc, eval_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearrange)\n","  dataset = dataset.repeat(1)\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","  \n","def rearrange(input, output, target):\n","  features = {'input': input, 'output': output}\n","  return features, target\n"],"execution_count":498,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is-GhUDN62xC"},"source":["* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n","* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"]},{"cell_type":"code","metadata":{"id":"jCfwWXhb64Cc","executionInfo":{"status":"ok","timestamp":1670257601504,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["def pred2string(value, dictionary):\n","  sentence_string = []\n","  is_finished = False\n","\n","  for v in value:\n","    sentence_string = [dictionary[index] for index in v['index']]\n","\n","  answer = \"\"\n","  for word in sentence_string:\n","    if word == END:\n","      if_finished = True\n","      break\n","    \n","    if word != PAD and word != END:\n","      answer += word\n","      answer += \" \"\n","\n","  return answer, is_finished"],"execution_count":499,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwp9Nnwz7UoG"},"source":["* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n","* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"]},{"cell_type":"code","metadata":{"id":"-T536MdU7Taq","executionInfo":{"status":"ok","timestamp":1670257752678,"user_tz":-540,"elapsed":151183,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["import pandas as pd\n","\n","tokenize_as_morph = True\n","\n","data_path = 'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv'\n","\n","char2idx, idx2char, len_vocab = load_vocabulary(data_path)\n","train_input, train_label, eval_input, eval_label = load_data(data_path)"],"execution_count":500,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cVd7AOKinqn"},"source":["### 모델 구성"]},{"cell_type":"markdown","metadata":{"id":"hqLJ0a6r49yi"},"source":["* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"]},{"cell_type":"code","metadata":{"id":"CNeeXoZginvj","executionInfo":{"status":"ok","timestamp":1670257752679,"user_tz":-540,"elapsed":17,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["from tensorflow_estimator.python.estimator.model_fn import EstimatorSpec\n","def model(features, labels, mode, params):\n","  TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n","  EVAL = mode == tf.estimator.ModeKeys.EVAL\n","  PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n","\n","  position_encode = positional_encoding(params['embedding_size'], params['max_len'])\n","  if params['xavier_initializer']:\n","    embedding_initializer = 'glorot_normal'\n","  else:\n","    embedding_initializer = 'uniform'\n","\n","  embedding = tf.keras.layers.Embedding(params['len_vocab'], \n","                                        params['embedding_size'],\n","                                        embeddings_initializer=embedding_initializer)\n","  \n","  x_embedded_matrix = embedding(features['input']) + position_encode\n","  y_embedded_matrix = embedding(features['output']) + position_encode\n","\n","  encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n","                            params['attention_head_size'], params['layer_size'])\n","  decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'], \n","                            params['ffn_hidden_size'],\n","                            params['attention_head_size'], params['layer_size'])\n","  logits = tf.keras.layers.Dense(params['len_vocab'])(decoder_outputs)\n","  predict = tf.argmax(logits, 2)\n","\n","  if PREDICT:\n","    predictions = {'indexs': predict,\n","                   'logits': logits}\n","    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  labels_ = tf.one_hot(labels, params['len_vocab'])\n","  loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n","  accuracy = tf.compat.v1.metrics.accuracy(labels=labels, prediction=predict)\n","\n","  matrics = {'accuracy': accuracy}\n","  tf.summary.scalar('accuracy', accuracy[1])\n","\n","  if EVAL:\n","    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metrics_ops=matrics)\n","  assert TRAIN\n","\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","  train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\n","  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"],"execution_count":501,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H7PrLEWE1JCs"},"source":["### 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"Gy_Opm_A7DKC"},"source":["*   필요한 각종 인자들을 설정\n","*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"]},{"cell_type":"code","metadata":{"id":"CKGYuqmH6_kj","executionInfo":{"status":"ok","timestamp":1670257752680,"user_tz":-540,"elapsed":17,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["max_len = 25\n","epoch = 5000\n","batch_size = 256\n","embedding_size = 100\n","model_hidden_size = 100\n","ffn_hidden_size = 100\n","attention_head_size = 100\n","lr = 0.001\n","layer_size = 3\n","xavier_initializer = True"],"execution_count":502,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaXalEy57ODq"},"source":["*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n","*   평가 데이터에도 동일하게 가공"]},{"cell_type":"code","metadata":{"id":"NWlgWWIq1KSh","executionInfo":{"status":"ok","timestamp":1670257906574,"user_tz":-540,"elapsed":153910,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n","train_output_enc, train_output_dec_length = dec_output_processing(train_label, char2idx)\n","train_target_dec = dec_target_processing(train_label, char2idx)"],"execution_count":503,"outputs":[]},{"cell_type":"code","source":["eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx)\n","eval_output_enc, eval_output_dec_length = dec_output_processing(eval_label, char2idx)\n","eval_target_dec = dec_target_processing(eval_label, char2idx)"],"metadata":{"id":"EddTWN-9ZTMY","executionInfo":{"status":"ok","timestamp":1670257971955,"user_tz":-540,"elapsed":65385,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"execution_count":504,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZGgZzWs7Mr7"},"source":["* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n","* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"]},{"cell_type":"code","metadata":{"id":"B9vjc3Ck7F4J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670257971955,"user_tz":-540,"elapsed":15,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}},"outputId":"9958536d-8cfe-48e0-e029-128ee273808a"},"source":["transformer = tf.estimator.Estimator(\n","    model_fn = model,\n","    params = {'embedding_size': embedding_size,\n","              'model_hidden_size': model_hidden_size,\n","              'ffn_hidden_size': ffn_hidden_size,\n","              'attention_head_size': attention_head_size,\n","              'learning_rate': lr,\n","              'len_vocab': len_vocab,\n","              'layer_size': layer_size,\n","              'max_len': max_len,\n","              'xavier_initializer': xavier_initializer}\n",")"],"execution_count":505,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmps3xs96ug\n"]}]},{"cell_type":"markdown","metadata":{"id":"wl_pwUiw7INZ"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"COO-0PcS7Hy5","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1670257975300,"user_tz":-540,"elapsed":3348,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}},"outputId":"11a03806-3e56-454d-b64a-b6233b87db35"},"source":["transformer.train(input_fn=lambda: train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size), steps=epoch)\n","eval_result = transformer.evaluate(input_fn=lambda: eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size))\n","print(\"{accuracy: 0.3f}\".format(**eval_result))"],"execution_count":506,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-506-a3f5285fb841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_output_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_target_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{accuracy: 0.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1184\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1212\u001b[0m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[1;32m   1213\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0m\u001b[1;32m   1215\u001b[0m                                            self.config)\n\u001b[1;32m   1216\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-501-d0875304b50f>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m     20\u001b[0m   encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n\u001b[1;32m     21\u001b[0m                             params['attention_head_size'], params['layer_size'])\n\u001b[0;32m---> 22\u001b[0;31m   decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'], \n\u001b[0m\u001b[1;32m     23\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ffn_hidden_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                             params['attention_head_size'], params['layer_size'])\n","\u001b[0;32m<ipython-input-487-f555755a825f>\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layer)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffn_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-487-f555755a825f>\u001b[0m in \u001b[0;36mdecoder_module\u001b[0;34m(inputs, encoder_outputs, model_dim, ffn_dim, heads)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecoder_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffn_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmasked_self_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msublayer_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   self_attn = sublayer_connection(masked_self_attn, \n\u001b[1;32m      4\u001b[0m                                   multi_head_attention(masked_self_attn,\n\u001b[1;32m      5\u001b[0m                                                        \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-484-6c2f8052ae90>\u001b[0m in \u001b[0;36mmulti_head_attention\u001b[0;34m(query, key, value, num_units, heads, masked)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mattention_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-483-22c7bc42e87b>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(query, key, value, masked)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdiag_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearOperatorBlockLowerTriangular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpaddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/linalg/linear_operator_block_lower_triangular.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, operators, is_non_singular, is_self_adjoint, is_positive_definite, is_square, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Validate operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mcheck_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_proper_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moperators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m       \u001b[0mcheck_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_proper_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/check_ops.py\u001b[0m in \u001b[0;36massert_proper_iterable\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    451\u001b[0m   )\n\u001b[1;32m    452\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munintentional_iterables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;34m'Expected argument \"values\" to be a \"proper\" iterable.  Found: %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         type(values))\n","\u001b[0;31mTypeError\u001b[0m: Expected argument \"values\" to be a \"proper\" iterable.  Found: <class 'tensorflow.python.framework.ops.Tensor'>"]}]},{"cell_type":"code","source":["def chatbot(question):\n","  pred_input_enc, pred_input_enc_length = enc_processing([question], char2idx)\n","  pred_output_dec, pred_output_dec_length = dec_output_processing([\"\"], char2idx)\n","  pred_target_dec = dec_target_processing([\"\"], char2idx)\n","\n","  for i in range(max_len):\n","    if i > 0:\n","      pred_output_dec, pred_output_dec_length = dec_output_processing([answer],  char2idx)\n","      pred_target_dec = dec_target_processing([answer], char2idx)\n","\n","    predictions = transformer.predict(input_fn=lambda: eval_input_fn(pred_input_enc, pred_output_dec, pred_target_dec, 1))\n","\n","    answer, finished = pred2string(predictions, idx2char)\n","\n","    if finished:\n","      break\n","  return answer"],"metadata":{"id":"l_b7GIlzinNe","executionInfo":{"status":"aborted","timestamp":1670257975302,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNcrVf2z1LSM"},"source":["### 예측"]},{"cell_type":"markdown","metadata":{"id":"R5lY9DrW8eSK"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"N9IQaBx4Qw8J","executionInfo":{"status":"aborted","timestamp":1670257975303,"user_tz":-540,"elapsed":13,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjHZKvJ31MAU","executionInfo":{"status":"aborted","timestamp":1670257975303,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["chatbot(\"안녕?\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mjRZwyLQ_gP","executionInfo":{"status":"aborted","timestamp":1670257975304,"user_tz":-540,"elapsed":13,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":["chatbot(\"오늘 기분 어때?\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7AJCsXRTqJx","executionInfo":{"status":"aborted","timestamp":1670257975306,"user_tz":-540,"elapsed":15,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M8mfoUfeAWQ","executionInfo":{"status":"aborted","timestamp":1670257975306,"user_tz":-540,"elapsed":15,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5mrdGRaem6v","executionInfo":{"status":"aborted","timestamp":1670257975306,"user_tz":-540,"elapsed":15,"user":{"displayName":"‍심서현[ 학부휴학 / 영어영문학과 ]","userId":"03317337495555700739"}}},"source":[],"execution_count":null,"outputs":[]}]}